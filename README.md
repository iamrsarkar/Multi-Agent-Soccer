# ğŸ® Multi-Agent-Soccer(Competitive Reinforcement Learning)

## ğŸ“˜ Overview
This project implements **competitive multi-agent reinforcement learning (MARL)** for game-like environments such as **soccer**.  
Each agent independently learns to **cooperate with teammates** and **compete against opponents** using **self-play** and **policy gradient** methods.

The project demonstrates emergent teamwork, strategy formation, and adaptive play dynamics in simulated multi-agent environments.

---

## ğŸ¯ Objective
Develop an **autonomous game AI system** in which multiple agents learn to:
- Compete against each other using reinforcement learning.  
- Develop cooperative team strategies in a shared environment.  
- Improve through **self-play** and **league training** (similar to DeepMindâ€™s AlphaStar).  

---

## ğŸ§© Concept
Each player or unit is modeled as an **independent agent** that:
- Observes the game state (e.g., position, velocity, ball location).
- Chooses an action (move, shoot, pass, defend).
- Receives a reward based on performance (goals, captures, wins).  

Agents train via **multi-agent policy gradients**, using **centralized training with decentralized execution** (CTDE).  

---

## ğŸ—ï¸ Environment Setup

### ğŸ”¹ Example Environments
- **âš½ Soccer (2v2)** â€“ agents learn to score and defend.
- **ğŸš© Capture-the-Flag** â€“ two teams try to capture the opponentâ€™s flag.
- **ğŸ“ Pong-Team** â€“ cooperative paddle control to keep the ball in play.
- **ğŸ¾ PettingZoo Envs:** `simple_spread`, `multiwalker`, `pistonball`.

---

### ğŸ”¹ Observations
Each agent observes:
- Its own position, velocity, orientation.
- Relative positions of teammates, opponents, and objectives (e.g., ball or flag).
- Global game features (time left, score).

### ğŸ”¹ Actions
Continuous or discrete action space:
- Move Up / Down / Left / Right
- Pass / Shoot / Defend / Idle  

### ğŸ”¹ Rewards
Example reward shaping (Soccer):
\[
R_t = R_\text{goal} + R_\text{teamwork} - R_\text{foul} - R_\text{distance}
\]
Where:
- \( R_\text{goal} = +1 \) per goal scored  
- \( R_\text{teamwork} = +0.1 \) for successful passes  
- \( R_\text{foul} = -0.5 \) for collisions or going out of bounds  
- \( R_\text{distance} = -\text{dist(ball, goal)} \) for shaping movement  

---

## âš™ï¸ Algorithms Implemented

| Algorithm | Description | Application |
|------------|--------------|--------------|
| **Self-Play PPO** | Agents train by competing with versions of themselves | Core training loop |
| **League Training** | Multiple policy pools compete and evolve (AlphaStar-style) | Advanced training |
| **Centralized Critic, Decentralized Actors** | Shared value estimation for cooperativeâ€“competitive balance | Stability in multi-agent updates |
| **Curriculum Learning** | Gradually increases difficulty (1v1 â†’ 2v2) | Robust policy formation |

---

## ğŸ§  Architecture

### Training Flow


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize environment (PettingZoo/Unity) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each episode: â”‚
â”‚ â€¢ Agents observe environment â”‚
â”‚ â€¢ Take actions using current policy â”‚
â”‚ â€¢ Environment updates game state â”‚
â”‚ â€¢ Compute rewards for all agents â”‚
â”‚ â€¢ Store experiences (state, action, reward) â”‚
â”‚ â€¢ Update policies via PPO or League strategy â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


### Directory Structure


â”œâ”€â”€ envs/ # Game environments (PettingZoo or Unity)
â”‚ â”œâ”€â”€ soccer_env.py
â”‚ â”œâ”€â”€ capture_flag_env.py
â”‚ â””â”€â”€ pong_team_env.py
â”œâ”€â”€ agents/ # RL agent implementations
â”‚ â”œâ”€â”€ ppo_agent.py
â”‚ â”œâ”€â”€ selfplay_manager.py
â”‚ â””â”€â”€ centralized_critic.py
â”œâ”€â”€ training/ # Training & evaluation loops
â”‚ â”œâ”€â”€ train_selfplay.py
â”‚ â””â”€â”€ league_training.py
â”œâ”€â”€ results/ # Logs, graphs, and replay files
â”œâ”€â”€ models/ # Trained checkpoints
â””â”€â”€ main.py # Entry point


---

## ğŸ§© Frameworks & Libraries

- ğŸ§  **Reinforcement Learning:** PyTorch, Stable-Baselines3, RLlib  
- ğŸ•¹ï¸ **Simulation Environments:** PettingZoo, Gymnasium, Unity ML-Agents  
- ğŸ“Š **Visualization:** Matplotlib, TensorBoard  
- âš™ï¸ **Physics (optional):** PyBullet or Mujoco  

---

## ğŸ“ˆ Evaluation Metrics

| Metric | Description |
|---------|--------------|
| **Win Rate** | % of matches won by agent/team |
| **Goal Difference** | Average goals scored âˆ’ conceded |
| **Average Reward** | Mean episode reward |
| **Policy Entropy** | Diversity in learned strategies |
| **Training Stability** | Reward variance across episodes |

---

## ğŸ® Experiments

| Experiment | Goal | Setup |
|-------------|------|-------|
| 1 | Train 1v1 Self-Play PPO | Baseline |
| 2 | Add Team Coordination (2v2 Soccer) | Shared rewards |
| 3 | League Training with Evolving Opponents | AlphaStar-style |
| 4 | Curriculum Difficulty (Easy â†’ Hard Maps) | Progressive learning |

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install Dependencies
```bash
conda create -n marl_game python=3.10
conda activate marl_game
pip install torch gymnasium pettingzoo stable-baselines3 matplotlib

2ï¸âƒ£ Train Agent
python main.py --env soccer --algo selfplay_ppo --episodes 10000

3ï¸âƒ£ Evaluate Policy
python evaluate.py --model models/soccer_ppo_final.pth

4ï¸âƒ£ Visualize Results
python visualize.py --env soccer

ğŸ“Š Visualization

Training Curves (Average Reward, Win Rate)

Agent Trajectories

Replay Videos (if using Unity ML-Agents)

ğŸ§© Research Extensions

Add Graph Neural Networks (GNN) for agent communication.

Explore Opponent Modeling (explicit opponent policy prediction).

Combine Self-Play + Imitation Learning (for human-like strategies).

Integrate League ELO rating for opponent matchmaking.

ğŸ“š References

Silver et al., â€œMastering the Game of Go with Deep Neural Networks and Tree Search,â€ Nature, 2016.

Vinyals et al., â€œGrandmaster Level in StarCraft II using Multi-Agent Reinforcement Learning,â€ Nature, 2019 (AlphaStar).

PettingZoo: Multi-Agent Reinforcement Learning Environment Library.

Schulman et al., â€œProximal Policy Optimization (PPO),â€ 2017.

ğŸ‘¨â€ğŸ’» Contributors

Ronak Sarkar â€“ Project Lead, Multi-Agent RL Researcher

Group RR â€“ Team Members (Radheshyam Routh, Ronak Sarkar)

MSc Big Data Analytics, RKMVERI (2024â€“2026)

ğŸª™ License

MIT License Â© 2025 Ronak Sarkar
You are free to use, modify, and distribute this code with proper attribution.

ğŸ–¼ï¸ Example Simulation Snapshot


---

Would you like me to **generate this README.md file (downloadable)** or also create the **project folder structure with stub `.py` files** so you can directly initialize it as a GitHub repo (with working placeholders for PettingZoo + PPO integration)?

