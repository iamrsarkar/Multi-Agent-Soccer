ğŸ® Multi-Agent 3v3 Soccer (Competitive Reinforcement Learning)
ğŸ“˜ Overview

This project implements a 3 vs 3 competitive multi-agent reinforcement learning (MARL) soccer simulation.
Each of the six players is controlled by its own learned policy and competes in a dynamic soccer environment using self-play PPO.

The project demonstrates:

Emergent teamwork

Strategy formation

Competitive & cooperative behavior

A live UI match viewer where agents play continuously until manually closed

ğŸ¯ Objective

Build an autonomous 3v3 soccer AI where:

6 independent agents learn to play soccer competitively

Agents train via self-play and learn offensive + defensive strategies

The trained model can be evaluated in a visual soccer field UI

The UI runs continuously until the user closes the window/terminal

ğŸ§© Concept
Each agent:

Observes:

Its position, velocity

Ball position

Teammates & opponentsâ€™ positions

Game score and time

Acts:

Move Up / Down / Left / Right

Dash / Sprint

Kick / Pass

Idle

Receives rewards based on:

Goals scored

Successful passes

Defensive stops

Ball possession

Fouls or collisions

Training uses:

Centralized critic, decentralized actors (CTDE)

Self-play PPO

Optional league training later

ğŸŸï¸ 3v3 Soccer Environment Setup
Environment Features

3 Agents vs 3 Agents

Continuous 2D Soccer Field

Physics-based ball movement

Collision detection

Reward shaping for passes, goals, possession

Built using PettingZoo ParallelEnv API

Observations (per agent)

[x, y, vx, vy] of the agent

[x, y] of ball

[x, y] of teammates

[x, y] of opponents

Actions
Action	Meaning
0	Move Up
1	Move Down
2	Move Left
3	Move Right
4	Dash
5	Kick
6	Pass
7	Idle
âš™ï¸ Algorithms Implemented
Algorithm	Description
Self-Play PPO	Agents train by playing against copies of themselves
Centralized Critic	One shared critic for stability
Decentralized Actors	Independent action policies
Curriculum Learning	Start with simple ball-chasing â†’ full 3v3
ğŸ§  Training Architecture
Training Flow
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Initialize 3v3 soccer environment                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ For each episode:                                       â”‚
â”‚   â€¢ All 6 agents observe state                          â”‚
â”‚   â€¢ Agents take actions via PPO policy                  â”‚
â”‚   â€¢ Environment updates physics and ball movement       â”‚
â”‚   â€¢ Rewards assigned (goals, passes, possession, etc.)  â”‚
â”‚   â€¢ Store transitions in replay buffer                  â”‚
â”‚   â€¢ PPO update occurs after rollout length              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“ Directory Structure
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ soccer_env_3v3.py       # 3v3 soccer simulation
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ppo_agent.py            # PPO decentralized actors
â”‚   â”œâ”€â”€ centralized_critic.py   # Shared critic network
â”‚   â””â”€â”€ selfplay_manager.py     # Self-play policy handling
â”‚
â”œâ”€â”€ training/
â”‚   â””â”€â”€ train_selfplay.py       # Main training loop
â”‚
â”œâ”€â”€ evaluation/
â”‚   â””â”€â”€ evaluate_match.py       # Runs UI 3v3 match viewer
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ soccer_viewer.py        # Live UI using pygame
â”‚
â”œâ”€â”€ results/                    # logs, graphs, training curves
â”œâ”€â”€ models/                     # PPO saved weights
â””â”€â”€ main.py                     # CLI runner

ğŸ® Live UI Viewer (3v3 Soccer)

After training, you can visualize the match where:

All 6 agents appear on the field

Ball moves based on physics

Scoreboard updates in real-time

Agents move, pass, defend

The UI stays open until you close the window / kill the terminal

The UI is built using pygame.

ğŸš€ How to Run the Project
1ï¸âƒ£ Install Dependencies
conda create -n marl_soccer python=3.10 -y
conda activate marl_soccer

pip install torch gymnasium pettingzoo stable-baselines3 pygame tensorboard matplotlib

2ï¸âƒ£ Train the 3v3 Soccer Agents
python training/train_selfplay.py \
    --episodes 5000 \
    --rollout-length 256 \
    --log-dir results/tensorboard \
    --checkpoint-dir models \
    --save-interval 100

3ï¸âƒ£ Evaluate the Trained Model (Runs the UI)
python evaluation/evaluate_match.py --model models/soccer_ppo_final.pth


â¡ï¸ This will open a soccer field UI showing all 6 agents playing.
â¡ï¸ The match continues until you manually close the pygame window or press CTRL+C.

ğŸ“ˆ Evaluation Metrics
Metric	Meaning
Win Rate	Percent of matches won vs. previous policies
Goals Scored	Number of goals per episode
Pass Accuracy	% of completed passes
Possession Time	Ball control percentage
Reward Stability	Convergence of PPO training
ğŸ“š References

PettingZoo MARL Framework

PPO (Schulman et al., 2017)

AlphaStar (DeepMind, 2019)

Multi-Agent RL (Lowe et al., MADDPG, 2017)

ğŸ‘¨â€ğŸ’» Contributors

Ronak Sarkar â€“ Project Lead (RL + MARL + Simulation)

Group RR â€“ Supporting Research and Development

ğŸª™ License

MIT License Â© 2025 Ronak Sarkar

You may use, modify, and distribute this work with attribution.
